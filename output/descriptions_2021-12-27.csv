descriptions,index
"If you are a FULLY REMOTE - Data Engineer with experience, please read on!What You Need for this Position Data Analysis Data Warehouse ETL Azure Linux Python System Administration GolangSo, if you are a FULLY REMOTE - Data Engineer with experience, please apply today!Colorado employees will receive paid sick leave. For additional information about available benefits, please contact Justin SandyEmail Your Resume In Word ToLooking forward to receiving your resume through our website and going over the position with you. Clicking apply is the best way to apply, but you may also:Justin.Sandy@CyberCoders.comPlease do NOT change the email subject line in any way. You must keep the JobID: linkedin : JS30-1664622 -- in the email subject line for your application to be considered.***Justin Sandy - Recruiter - CyberCodersApplicants must be authorized to work in the U.S.CyberCoders, Inc is proud to be an Equal Opportunity EmployerAll qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, protected veteran status, or any other characteristic protected by law.Your Right to Work - In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire.CyberCoders will consider for Employment in the City of Los Angeles qualified Applicants with Criminal Histories in a manner consistent with the requirements of the Los Angeles Fair Chance Initiative for Hiring (Ban the Box) Ordinance.",0
"Every month, billions of people leverage Facebook products to connect with friends and loved ones from across the world. On the Data Engineering Team, our mission is to support these products both internally and externally by delivering the best data foundation that drives impact through informed decision making. As a highly collaborative organization, our data engineers work cross-functionally with software engineering, data science, and product management to optimize growth, strategy, and experience for over three billion users, as well as our internal employee community. In this role, you will see a direct correlation between your work, company growth, and user satisfaction. Beyond this, you will work with some of the brightest minds in the industry, and you'll have a unique opportunity to solve some of the most interesting data challenges around efficiency and integrity, at a scale few companies can match. As we continue to expand and create, we have a lot of exciting work ahead of us!Manage and execute data warehouse plans for a product or a group of products to solve well-scoped problems.Identify the data needed for a business problem and implement logging required to ensure availability of data, while working with data infrastructure to triage issues and resolve.Collaborate with engineers, product managers and data scientists to understand data needs, representing key data insights visually in a meaningful way.Build data expertise and leverage data controls to ensure privacy, security, compliance, data quality, and operations for allocated areas of ownership.Design, build and launch new data models and visualizations in production, leveraging common development toolkits.Independently design, build and launch new data extraction, transformation and loading processes in production, mentoring others around efficient queries.Support existing processes running in production and implement optimized solutions with limited guidance.Define and manage SLA for data sets in allocated areas of ownership.Experience in the data warehouse space.Experience in custom ETL design, implementation and maintenance.Experience with object-oriented programming languages.Experience with schema design and dimensional data modeling.Experience in writing SQL statements.Experience analyzing data to identify deliverables, gaps and inconsistencies.Experience managing and communicating data warehouse plans to internal clients.Bachelor's degree in Computer Science, Computer Engineering, relevant technical field, or equivalent practical experienceExperience working with either a MapReduce or an MPP system.Knowledge and practical application of Python.Facebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities - we're just getting started.Facebook is proud to be an Equal Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law.Facebook is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com.",1
"Panjiva is a data-driven technology company that uses machine learning to provide powerful search, analysis, and visualization of billions of shipping records from nearly every country in the world. More than 3,000 customers in over 100 countries, ranging from Fortune 500 companies and startups to government agencies and hedge funds, rely on our platform for supply chain intelligence. In global trade, better insight means better decision making and stronger connections between companies and governments across the globe.Recognizing Panjiva’s cutting-edge technology, S&P Global acquired Panjiva in 2018. This acquisition has grown our resources, dramatically expanded our access to data, and accelerated our growth plans.People are Panjiva’s greatest strength – join our engineering team as we map out a key part of the world economy!Job DescriptionAs a data engineer on our team, you will play a key role in developing our next-generation data science infrastructure and underlying core technologies. You will work with Panjiva’s world-class data scientists, analysts, and engineers to create products that solve important real-world business problems in a collaborative, fast-paced, and fun environment.You’ll work closely with our data science team to develop new platforms, infrastructure, and tools that will allow for machine learning applications at production scale over ever-growing datasets.You’ll design and leverage distributed computing technologies, data schemas, and APIs to construct data science pipelines. In addition, you’ll be expected to participate in augmenting our infrastructure to seamlessly integrate new data sets through constant R&D of the technologies and systems we use.Join us in building the next generation of products as we continue to deliver valuable and actionable insights to decision-makers in the $15 trillion global trade industry.ResponsibilitiesArchitect and implement distributed systems that perform complex transformations, processing, and analysis over very large scale datasetsDevelop processes to monitor and automate detection of quality regressions in raw data or in the output of Panjiva’s machine learning modelsWorking with our data scientists to turn large-scale messy, diverse, and often unstructured data into a source of meaningful insights for our customersOptimizing slow-running database queries and data pipelinesHelping enhance our search engine, capable of running sophisticated user queries quickly and efficientlyBuilding internal tools and backend services to enable our data scientists and product engineers to improve efficiencyCompensation/Benefits InformationS&P Global states that the anticipated base salary range for this position is $91,500 to $190,100. Base salary ranges may vary by geographic location.This role is eligible to receive S&P Global benefits. For more information on the benefits we provide to our employees, visit https://www.spgbenefitessentials.com/newhires.QualificationsB.S., M.S., or Ph.D. in Computer Science (or a related field) or equivalent work experience3+ years of experience working with data-at-scale in a production environmentExperience designing and implementing large-scale, distributed systemsExperience in multi-threaded software development (or some form of parallelism)Significant performance engineering experience (e.g., profiling slow code, understanding complicated query plans, etc.)Solid understanding of core algorithms and data structures, including the ability to select (and apply) the optimal ones to computationally expensive operations over data-at-scaleStrong understanding of relational databases and proficiency with SQLDeep knowledge of at least one scripting language (e.g., Python, Ruby, JavaScript)Deep knowledge of at least one compiled language (e.g., Scala, C++, Java, Go)Experience developing software on Linux-based operating systemsExperience with distributed version control systemsNice-to-HavesFamiliarity with relational database internals (especially PostgreSQL)Proficiency with cloud computing platforms, specifically AWSWorking knowledge of probability & statisticsContributions to open-source softwareExperience building customer-centric productsGrade (relevant for internal applicants only): 11S&P Global is an equal opportunity employer committed to making all employment decisions without regard to race/ethnicity, sex, pregnancy, gender identity or expression, color, creed, religion, national origin, age, disability, marital status (including domestic partnerships and civil unions), sexual orientation, military veteran status, unemployment status, or any other basis prohibited by federal, state or local law. Only electronic job submissions will be considered for employment.If you need an accommodation during the application process due to a disability, please send an email to: EEO.Compliance@spglobal.com and your request will be forwarded to the appropriate person.The EEO is the Law Poster http://www.dol.gov/ofccp/regs/compliance/posters/pdf/eeopost.pdf describes discrimination protections under federal law.20 - Professional (EEO-2 Job Categories-United States of America), IFTECH202.2 - Middle Professional Tier II (EEO Job Group), SWP Priority – Ratings - (Strategic Workforce Planning)Job ID: 257140Posted On: 2021-03-25Location: Cambridge, Massachusetts, United States",2
"Want to build new features and improve existing products that more than a billion people around the world use? Are you interested in working on highly impactful technical challenges to help the world be more open and connected? Want to solve unique, large-scale, highly complex technical problems? Our development cycle is extremely fast, and we've built tools to keep it that way. It's common to write code and have it running live on the site just hours later. We push code to the site continuously and have small teams that build products that are touched by millions of people around the world. If you work for us, you will be able to make an impact immediately. Facebook is seeking Software Engineers to join our engineering team. As a Software Engineer at Facebook, you’ll drive the development of the systems behind Facebook's products, create web applications that reach billions of people, build high volume servers and be a part of a team that’s working to help people connect with each other around the globe. This position is full-time and there are minimal travel requirements.Develop a strong understanding of relevant product area, codebase, and/or systemsDemonstrate proficiency in data analysis, programming and software engineeringProduce high quality code with good test coverage, using modern abstractions and frameworksWork independently, use available resources to get unblocked, and complete tasks on-schedule by exercising strong judgement and problem solving skillsMaster Facebook’s development standards from developing to releasing code in order to take on tasks and projects with increasing levels of complexityActively seek and give feedback in alignment with Facebook’s Performance PhilosophyExperience coding in an industry-standard language (e.g. Java, Python, C++, JavaScript)Must obtain work authorization in country of employment at the time of hire, and maintain ongoing work authorization during employmentDemonstrated software engineering experience from previous internship, work experience, coding competitions, or publicationsCurrently has, or is in the process of obtaining, a Bachelors or Masters degree in Computer Science or a related fieldFacebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities - we're just getting started.Facebook is proud to be an Equal Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law.Facebook is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com.",3
"If you are a 100% REMOTE- Data Engineer (Healthcare) with experience, please read on!Top Reasons to Work with UsWe are making a major impact at the core of U.S. healthcare by implementing the latest in cutting-edge technologies. We sit right at the intersection of heathtech and fintech.The U.S. healthcare system suffers from over $300B in improper payments each year due to fraud, waste, abuse, and processing errors. Were on a mission to change that.Our team has alumni ranging from Amazon, Goldman Sachs, Cotiviti, the Centers for Medicare and Medicaid Services, and other leading healthcare and financial institutions. Were also backed by leading venture capital firms and a graduate of ERA!What You Will Be Doing Writing production-level code in languages such as Python and Rust for our data pipelines and adjacent services Designing new services to extract value in a real-time data ingestion context Architecting complex data pipelines using Apache Airflow and Kubernetes to enable human-in-the-loop machine learning models in production Innovating in data engineering by leveraging modern technologies such as Apache Arrow, Postgres Foreign Data Wrappers, Deltalake, and MLFlow Working closely with our backend and machine learning engineers to implement new APIs and services to power new in-app functionality Collaborating with our Product team to enable frontend features empowered by a robust data pipelineWhat You Need for this Position 5+ years of experience Experience building and deploying production data processing pipelines Orchestrators such as Airflow or Kubeflow At least one scripting language, preferably Python PostgreSQL Dataframe frameworks such as Pandas, Polars, Nushell, or Spark Datalakes such as Snowflake, Deltalake, Azure Synapse, or AWS Athena/GlueWhat's In It for You Competitive compensation Medical, Vision and Dental coverage Equity Flexible paid vacation policy Work in a flat organizational structure- direct access to LeadershipSo, if you are a 100% REMOTE- Data Engineer (Healthcare) with experience, please apply today!Colorado employees will receive paid sick leave. For additional information about available benefits, please contact Parker GrahamEmail Your Resume In Word ToLooking forward to receiving your resume through our website and going over the position with you. Clicking apply is the best way to apply, but you may also:Parker.Graham@CyberCoders.comPlease do NOT change the email subject line in any way. You must keep the JobID: linkedin : PG2-1661126 -- in the email subject line for your application to be considered.***Parker Graham - Lead Recruiter - CyberCodersApplicants must be authorized to work in the U.S.CyberCoders, Inc is proud to be an Equal Opportunity EmployerAll qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, protected veteran status, or any other characteristic protected by law.Your Right to Work - In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire.",4
"Dear Builders, Rule-Breakers & Adventurers: WE WANT YOUHere at Sugatan, we believe in co-creating with passionate builders who love building something extraordinary out of nothing purely driven by the love of our crafts.It all started with a ludicrous question. How can we turn $1 into $3? Through the relentless pursuit of the answer, we’ve since turned $62 million into $180 million to date, empowering humans, communities & movements throughout our journey.The Sugatan culture is all about cultivating unique experiences collectively as a superorganism as we face a multitude of challenges together whilst profits are reinvested back into funding exciting projects we love. It’s a self-evolving, self-realizing organization that paves the path for others to follow. So are you ready to step up and step into your element? Walk with us.Duties & Responsibilities:Expanding and optimizing our data and data pipeline architectureWork across all functional leads creating and/or implementing methods to improve data reliability and quality as well as optimizing data flow and collectionBuild tools to provide actionable insights into key business performance metrics to optimize our services and create value for our clientsAutomating manual processes to support our current and future data initiativesAssemble large, complex data sets that meet analytics and business requirements Apply best practices that enable a quality product while keeping our data separated and secureQualifications:Must have minimum 3 years of experience as a data pipeline builder who enjoys optimizing data systems and building them from the ground upExpert of SQL and relational databases and/or AWS cloud services Experience with Python, Java, C++, Scala, etc. Bachelor’s Degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative discipline or have proven work experience in a relevant positionHighly organized innovator, critical thinker, and problem solverMust work USA, Eastern Standard Time Self-managing and has no issues quoting the right time frames for delivery and hitting deadlines Minimum 6 months experience working in Digital MarketingExperience working with eCommerce businesses a plus",5
"About Our CompanyVytalize Health is a leading provider of value-based care for Medicare patients. We power independent primary care practices through bold financial incentives and smart technology. We are on a mission to accelerate the world’s transition to value-based care by taking care of the doctors who take care of us. We are the fastest growing value-based care provider in the country spanning 14 states and more than 1,000 PCPs.Visit www.vytalizehealth.com for more information.Job DescriptionThe Network Development Data Analyst will be working closely with our management team to pull information from our system of choice Trella Health. This role will also collect other publicly available data to include in reports.Using data mining to extract information from data sets and identify correlations and patterns Organising and transforming information into comprehensible structuresPerforming statistical analysis of dataUsing tools and techniques to visualise data in easy-to-understand formats, such as diagrams and graphsPreparing reports and presenting these to management Monitoring data quality and removing corrupt dataCandidate DescriptionThe ideal candidate has experience utilizing publicly available data alongside system data to help aggregate. A keen eye for detail along with the ability to complete projects within deadlines is a must. Being able to turn the data into information, information into insight and insight into business decisions is what we are looking for.Educational RequirementsBachelor’s degree in Computer Science, Engineering, Mathematics or other technical discipline (proven professional experience will be considered in lieu of a technical degree)Job RequirementsExperience with Trella Health highly preferableMust be Proficient in Microsoft Excel and Access Analytics: 2 years (Required)Data Analyst: 2 years (Required) Ability to work independently and collaborativelyExcellent written and verbal communication skillsAbility to effectively prioritize and execute tasks in a fast moving environmentHigh attention to detailJob Type: Full-time- RemoteSalary: $75,000- $95,000 base salary",6
"Senior Analytics Engineer (FULLY REMOTE)This Jobot Job is hosted by Sam MaldonadoAre you a fit? Easy Apply now by clicking the ""Apply"" button and sending us your resume.Salary $160,000 - $170,000 per yearA Bit About UsOur mission is to help and support families and our elderly through the journey of aging. Our company provides a variety of services like assistance with technology, assistance with transportation, general help, love, support, and friendship in order to maximize the best of life and to minimize depression and other health problems our elderly can face. We provide care to millions of families and our mission is to help people feel more healthy, loved, and supported in order to maintain the best lives possible in the later stages of life.Why join us?Strong Competitive SalaryStock OptionsHealth Care, Vision, DentalPTO + Paid Holidays401k MatchingCareer Growth OpportunitiesJob DetailsMust HaveAWS ExperienceSQLNoSQLNon RelationalWe need someone with a breadth of experience in a variety of databases (sql and nosql/relational and non relational), cloud and someone who has a mixture of analytics and engineering (more engineering but not as heavy as say a Data Engineer).Good Pluses to HaveLookerLookMLETLPythonRInterested in hearing more? Easy Apply now by clicking the ""Apply"" button.",7
"The Data Intelligence Group (DIG) is an important driver of BAM’s continued growth. Year over year, data plays an increasingly important role in the firm’s core businesses. The analysis, services, software, and operational expertise that DIG provides are a key part of BAM’s competitive advantage.Role OverviewWe are looking for creative and enthusiastic Software Engineers to join our team in building the best Data Platform on the street. We develop and maintain applications, APIs, and websites that help our investment teams use data to make better investment decisions. BAM’s portfolio managers, analysts and quants rely on us to provide analytics-ready data in an easily digestible format. In this role, you will:Develop services and integrate them with 3rd party APIsDesign schemas/storage patternsDeploy, Monitor, and Support applications on KubernetesCollaborate with data analysts Leverage AWS services for logging, monitoring, data storage, and moreGive and receive feedback in code reviewsParticipate in planning, scoping, and refinement exercises WHAT YOU’LL BRINGBachelors/Masters degree in Computer Science or a related field3+ years of professional experience with Python or NodeJS3+ years of professional experience with SQL/NoSQL databases2+ years of professional experience building applications with Docker on K8s1+ year of professional experience building pipelines with one Airflow, Luigi, Oozie, Nifi, or similarAbility to understand and contribute to our existing data system softwareStrong oral and written communication skills",8
"About Datadog:We're on a mission to build the best platform in the world for engineers to understand and scale their systems, applications, and teams. We operate at high scale—trillions of data points per day—allowing for seamless collaboration and problem-solving among Dev, Ops and Security teams globally for tens of thousands of companies. Our engineering culture values pragmatism, honesty, and simplicity to solve hard problems the right way.Students can sign up anytime for a free Datadog Pro account!The Opportunity:We’re looking for recent graduates to join us to help collect, aggregate, visualize, and analyze extremely high-scale metrics, logs, and application data. You'll join one of Datadog's software teams and work alongside other engineers to help solve complex problems. Whether you are writing a new integration, building data pipelines, or working with the open source community, our interns are key contributors to our product. This role will be based in our New York or Boston offices.In This Role You Will:Solve a scaling bottleneck in a critical service, deploy a new feature to production and progressively roll it out with feature flags, and investigate and fix a production issue from a service your team owns. You will also create a way to optimize a service to handle more traffic and in partnership, with your team, planning the most important projects to work on next.You Are:You are in the process of receiving/have a degree in Computer Science, Software Engineering, or a related fieldYou are targeting a 2022 full-time start dateYou are comfortable coding in one or more languagesYou value code simplicity and performanceYou have worked as a software engineering intern or have related industry experienceYou want to work in a fast-paced environment that values its employees and customersYou are a self-starter who enjoys partnering with other engineers and solving difficult problemsBonus Points:You have worked at high scale with systems like Redis, Cassandra, or KafkaYou wrote your own data pipelines once or twice beforeYou have significant experience with Go or PythonWhy You Should Apply:Mentorship opportunities with team members, leadership, and beyondContinuous career and professional development opportunitiesTraining to develop an in-depth understanding of our product and spaceRelocation stipend and travel benefitsIntern cohort and monthly company social eventsNetworking opportunities with across all teams at DatadogUnlimited snacks & drinks in our officeCatered Lunches on Monday, Wednesday, and FridayCommuter benefitsBeautiful offices in convenient locations with amazing viewsAs the COVID-19 crisis continues to change globally, our hiring plans will change with it. We are following current company safety guidelines with the goal to hire team members who will work within one of our offices (NYC, Boston, Paris) once it’s safe to do so. We will update all applicants with the appropriate changes when necessary.Equal Opportunity at Datadog:Datadog is an Affirmative Action and Equal Opportunity Employer and is proud to offer equal employment opportunity to everyone regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity, veteran status, and more. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements.Your Privacy:Any information you submit to Datadog as part of your application will be processed in accordance with Datadog’s Applicant and Candidate Privacy Notice.",9
"We're looking for a Data Engineer to join a company in the Underdog.io network.The Underdog.io network is a curated group of some of the fastest growing startups and tech companies in the country. We actively turn away more than 50% of companies that attempt to join.We accept companies that offer competitive salaries, benefits, and perks. They're working on interesting technical challenges and must be respectful of your time to stay active.Our companies look for Data Engineers proficient in Python, Java, SQL, Hadoop, C++ and more. The ideal candidate is passionate about building clean pipelines and maintaining data products relied on by many. Many of our companies are looking for mid-to-senior level talent, both individual contributors and managers.To apply to the network, we'll ask you to fill out a 60-second web form. It's absolutely free.If accepted, you'll hear directly from founders, hiring managers, and other key decision makers starting the following Monday. Our platform will hide your profile from your current employer.Apply today!Building an inclusive and diverse workplace is one of Underdog.io’s core values. We warmly welcome people of all backgrounds, experiences, and perspectives.C++,Python (Programming Language),Java,Data Products,Pipelines,Hadoop,MapReduce,SQL,Data Warehousing,Data Architecture",10
"We believe that difference sparks brilliance, so we welcome people and ideas from everywhere to join us in stretching what’s possible.At Tapestry, being true to yourself is core to who we are. When each of us brings our individuality to our collective ambition, our creativity is unleashed. This global house of brands – Coach, Kate Spade New York, Stuart Weitzman – was built by unconventional entrepreneurs and unexpected solutions, so when we say we believe in dreams, we mean we believe in making them happen. We’re always on a journey to becoming our best, but you can count on this: Here, your voice is valued, your ambitions are supported, and your work is recognized.Job Title: Sr. Data EngineerPrimary PurposeSupport Tapestry’s big data platform and the development of enterprise AI / ML solutionsThe successful individual will leverage their proficiency in Data Engineering to:Support development, automation, configuration management and deployment of our cloud-native data pipelines to support business critical operations and analytics.Design and advocate for modern data solutions using cloud native technologies, solid design principles, secure integration points, and automated deployments.Create tooling, structure, automation for new initiatives and existing data pipelinesAct as the technical expert on solutions built on Tapestry’s data platform and data operationsOwn code pipeline and service configuration management needs of the Big Data platform teamOwn development, test and production environments within Data Engineering eco-system.The Accomplished Individual Will PossessStrong ANSI SQL development skills (DML, DCL, DDL) Working knowledge of Linux and AWS ecosystemExperience working on Big data frameworks like Spark, hbase, Phoenix, Hadoop, Presto, HiveExperience with AWS data technologies like Glue, Athena, and EMRFundamental understanding of data structures and experience with distributed computing platformsJava/J2EE, Spring Framework, RESTful Services, Micro services, Java Testing Frameworks (JUnit etc.)Good Scripting knowledge - automation using python/bash/other scripting languageUnderstanding of SCM principles and experience with versioning tools like Git, BitBucket etc.Some exposure to NoSQL databases (Dynamo, Cassandra, Mongo etc.)Experience developing web based interactive visualization tools and dashboards for Analytics is a plus.Experience with build & IAC tools like Ant, Maven, Gradle, CMake, Jenkins, Cloud Formation, Terraform etc. is a plus.An Outstanding Professional Will Have3-5 years of experience building Data Engineering solutionsBachelor’s degree in Computer Science or a related field Strong ability to understand, document and communicate technical architectures, best practices, toolsets, and solutions.Collaborative skills to partner with other development teams and business analysts to deliver revenue impacting data infrastructure/platform solutionsExcellent communication skills (written and verbal, formal, and informal).Flexible and proactive/self-motivated working style with strong personal ownership. Strong and innovative approach to problem solving and finding solutions.Our Competencies for All EmployeesCourage: Doesn’t hold back anything that needs to be said; provides current, direct, complete, and “actionable” positive and corrective feedback to others; lets people know where they stand; faces up to people problems on any person or situation (not including direct reports) quickly and directly; is not afraid to take negative action when necessary.Creativity: Comes up with a lot of new and unique ideas; easily makes connections among previously unrelated notions; tends to be seen as original and value-added in brainstorming settings.Customer Focus: Is dedicated to meeting the expectations and requirements of internal and external customers; gets first-hand customer information and uses it for improvements in products and services; acts with customers in mind; establishes and maintains effective relationships with customers and gains their trust and respect.Dealing with Ambiguity: Can effectively cope with change; can shift gears comfortably; can decide and act without having the total picture; isn’t upset when things are up in the air; doesn’t have to finish things before moving on; can comfortably handle risk and uncertainty.Drive for Results: Can be counted on to exceed goals successfully; is constantly and consistently one of the top performers; very bottom-line oriented; steadfastly pushes self and others for results.Interpersonal Savvy: Relates well to all kinds of people, up, down, and sideways, inside and outside the organization; builds appropriate rapport; builds constructive and effective relationships; uses diplomacy and tact; can diffuse even high-tension situations comfortably.Learning on the Fly: Learns quickly when facing new problems; a relentless and versatile learner; open to change; analyzes both successes and failures for clues to improvement; experiments and will try anything to find solutions; enjoys the challenge of unfamiliar tasks; quickly grasps the essence and the underlying structure of anything.Our Competencies for All People ManagersStrategic Agility: Sees ahead clearly; can anticipate future consequences and trends accurately; has broad knowledge and perspective; is future oriented; can articulately paint credible pictures and visions of possibilities and likelihoods; can create competitive and breakthrough strategies and plans.Developing Direct Reports and Others: Provides challenging and stretching tasks and assignments; holds frequent development discussions; is aware of each person's career goals; constructs compelling development plans and executes them; pushes people to accept developmental moves; will take on those who need help and further development; cooperates with the developmental system in the organization; is a people builder.Building Effective Teams: Blends people into teams when needed; creates strong morale and spirit in their team; shares wins and successes; fosters open dialogue; lets people finish and be responsible for their work; defines success in terms of the whole team; creates a feeling of belonging in the team.Tapestry, Inc. is an equal opportunity and affirmative action employer and we pride ourselves on hiring and developing the best people. All employment decisions (including recruitment, hiring, promotion, compensation, transfer, training, discipline and termination) are based on the applicant’s or employee’s qualifications as they relate to the requirements of the position under consideration. These decisions are made without regard to age, sex, sexual orientation, gender identity, genetic characteristics, race, color, creed, religion, ethnicity, national origin, alienage, citizenship, disability, marital status, military status, pregnancy, or any other legally-recognized protected basis prohibited by applicable law. #LI_CE1 Visit Tapestry, Inc. at http://www.tapestry.com/",11
"About Us:DailyPay is the leader in the on-demand pay industry with an unrivaled technology platform, an unmatched list of blue-chip clients and an extensive list of industry awards. We are rewriting the invisible rules of finance by creating a new financial system. A financial system that is more equitable and inclusive, and benefits everyone. A financial system that enables workers to access their earned pay when they need it. We believe that money should move faster and smoother between employer and employee, between merchant and shopper, between financial institution and customer.We are a mission-driven company hyper-focused on designing technology that can build a better financial system and future. It’s no wonder that we are growing at an extraordinary pace. Now we are looking for people who are as passionate as we are about reimagining how money moves. If you’re willing to define new rules, change systems and lives, come join us at DailyPay.The Role:DailyPay is looking for a Data Analytics Engineer to join our Analytics Engineering group inside the DailyPay Data Team. The Analytics Engineering group is responsible for building the data infrastructure that underpins our data analytics and data products that are used cross-functionally inside the company (sales, marketing, operations, engineering, etc.) as well as by DailyPay partner companies. The team also ETLs internal and external data to help the Data Team provide insights about the payroll industry in general, as well as about personal finance and financial wellbeing.The mission of the Data Team is critical for continuous development and success of our product, understanding the needs of our customers and partners, and maintaining DailyPay’s leading role in the early wage access industry -- and our Data Analytics Engineers are instrumental in helping to realize this vision for DailyPay.If this opportunity excites you, we encourage you to apply even if you do not meet all of the qualifications.How You Will Make an Impact:Build and maintain company’s ETL and data pipelinesProduce, support and maintain data reports, analytics, metrics and dashboards for internal and external useDesign and implement data testing and scaling capabilitiesMaintain and optimize monitoring and alerting for company’s ETL and data pipelines, data warehouse, and analytics infrastructureOptimize database performance while reducing warehouse costs and development timesParticipate in code approvals and PR review process for company-wide analytics engineering effortsWhat You Bring to The Team:3+ years SQL experience; expert SQL capabilityFamiliarity with BI tools such as Tableau, Looker, or similarExcellent presentation and communication skills1+ years of dbt experience preferredExperience with Snowflake, Redshift, and ETL tools like Fivetran or Stitch is a plusPython experience is a plusWhat We Offer:Competitive compensationOpportunity for equity ownershipExceptional health, vision, and dental careEmployee Resource GroupsFun company outings and eventsUnlimited books from AmazonUnlimited PTO401K with company matchNo sponsorship is available for this position.DailyPay does not accept and will not review unsolicited resumes from search firms.As part of our commitment to health and safety, DailyPay requires all colleagues holding in-office positions, or who will be attending any in-person company meetings, be fully vaccinated against the Covid-19 virus unless there is a documented and approved medical or religious accommodation. As a condition of employment, prior to your start date, you will be required to submit proof of your vaccine status.DailyPay is committed to fostering an inclusive, equitable culture of belonging, grounded in empathy and respect, which values openness to opinions, awareness of lived experiences, fair treatment and access for all. We strive to build and develop diverse teams to create an organization where innovation thrives, where the full potential of each person is engaged, and their views, beliefs and values are integrated into our ways of working.We welcome people of all backgrounds to join us on our mission*. If you require reasonable accommodation for any aspect of the recruitment process, please send a request to peopleops@dailypay.com. All requests for accommodation will be addressed as confidentially as practicable._______________________________________________________________________________________DailyPay is an equal opportunity employer. All qualified applicants will receive consideration without regard to race, color, religion or creed, alienage or citizenship status, political affiliation, marital or partnership status, age, national origin, ancestry, physical or mental disability, medical condition, veteran status, gender, gender identity, pregnancy, childbirth (or related medical conditions), sex, sexual orientation, sexual and other reproductive health decisions, genetic disorder, genetic predisposition, carrier status, military status, familial status, or domestic violence victim status and any other basis protected under federal, state, or local laws.",12
"An early stage startup with secure revenue and funding is looking to build out their data team with several data engineers of all levels! 100% remote, but someone on the East Coast preferred!This Jobot Job is hosted by Christine McNamaraAre you a fit? Easy Apply now by clicking the ""Apply"" button and sending us your resume.Salary $120,000 - $200,000 per yearA Bit About UsWe are a small startup looking to double in size over the next year. Our bleeding edge solutions are helping change the way the insurance industry operates on a daily basis and we are looking for Data Engineers to help us achieve these goals. The role would be entirely greenfield development and would offer you the opportunity to be involved in the overall technical direction of our data and analytics environment. We have the luxury of offering you all of the ownership and impacts as a small startup, while also giving you the security of a stable organization.Why join us?Opportunity to build something from scratchMedical, dental, vision coverage401k with company matchFull remote work Exponential growth opportunitiesUnlimited PTOCompetitive compensation packagesJob DetailsRequirements3-10 years of professional work experienceExperience building out data pipelines from scratch (Python)Good exposure to data warehousing and ETL tools (Snowflake, Redshift, etc)Strong database experience with extensive experience using SQLExperience with cloud based technologies (Azure, AWS)Comfortable building out CICD pipelines and deploying code into productionVersion Control experience (Git)Extensive problem solving experience and self-starter attitudeNice to HavesBachelors in STEM related fieldExperience with Spark or other streaming technologiesDomain knowledge experience working with insurance related data setsExperience working on projects from end to endCustomer facing experience to gather requirements and deliver solutions effectivelyMachine Learning and/or AI experienceExperience working with structured and unstructured data setsMentoring junior level engineersInterested in hearing more? Easy Apply now by clicking the ""Apply"" button.",13
"Company OverviewGEP is a diverse, creative team of people passionate about procurement. We invest ourselves entirely in our client’s success, creating strong collaborative relationships that deliver extraordinary value year after year. Our clients include market global leaders with far-flung international operations, Fortune 500 and Global 2000 enterprises, leading government and public institutions.We deliver practical, effective services and software that enable procurement leaders to maximise their impact on business operations, strategy and financial performance. That’s just some of the things that we do in our quest to build a beautiful company, enjoy the journey and make a difference. GEP is a place where individuality is prized, and talent respected. We’re focused on what is real and effective. GEP is where good ideas and great people are recognized, results matter, and ability and hard work drive achievements. We’re a learning organization, actively looking for people to help shape, grow and continually improve us.Are you one of us?GEP is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, ethnicity, color, national origin, religion, sex, protected veteran status, disability status, or any other characteristics protected by federal, state or local law. We are committed to hiring and valuing a global diverse work team. GEP is proud to be an EEO/AA employer M/F/D/V.For more information please visit us on GEP.com or check us out on LinkedIn.com.What You Will DoYou will use modern big data technologies in cloud environment to help accelerate analytics and data science projects across GEP. You will work on the flow and supply of data used by GEPs's customers, analysts, consultants, data scientists and other engineering teams and contribute to a team that will build and operate a platform which will drive GEPs' next generation product suite.You will build and maintain the flow of data from source systems (RDBMS, real time streams, unstructured) into the data platform, processes that query the platform, APIs that serve users, and tools used for monitoring and maintenance, in a secure, mission critical environment.Responsibilities includeDesign and develop data pipelines, transformations and services.Collaborate with multiple data sources and consumers to enforce standard patterns, best practice and design principles.Build and maintain cutting edge frameworks and systems to support big data use casesAccess, source, cleanse, merge, ETL, and possibly re-model data from 1,000s of data sources both internal and external to GEPWhat You Should Bring6+ years of hands-on coding experience with ETL & data engineering tools, especially in big data environments – data intensive applicationsExperience with Apache Spark, Coding with SQL, tuning and troubleshooting etc is must.Experience with Kafka, ADF, Hadoop, Data Bricks and any other big data environment is s a strong plusExpert with Python or Scala with a focus on functional programmingUnderstanding of full SDLC & exposure to all stages of developmentExperience maintaining production applications, Logging, tracing, and application monitoring is requiredExposure to Azure or other Cloud platforms is required.",14
"ROLE OVERVIEW: Balyasny Asset Management is seeking an experienced developer to work within the Macro Market Data team to build out next generation cloud native data initiatives that will underpin first-class analytics, trading tools and research. The optimal candidate will have a minimum of 2 years’ hands-on development experience with an excellent technical background and enjoy working with data. The candidate will be experienced in processing and manipulating data, developing scalable and performant data delivery systems, data APIs and platforms. This person will need to be a very strong communicator, able to multi-task and have the ability to excel in a fast-paced trading environment. The candidate would be comfortable leading projects from start to finish, communicating with stakeholders and ensuring success. In the role of Macro Data Engineer, the employee will be responsible for the following:Developing a keen understanding of the data and financial domain and how it is being utilized in our systems, analytics, and investment processesDeveloping and architecting next generation cloud native data platforms, pipelines and APIs within the Macro space using the AWS suite of technologies capable of delivering large volumes of data in a performant mannerWorking with a variety of vendors, including Bloomberg, Refinitiv, and delivering data across the full landscape of market data; from streaming real-time data to static, reference and historical data.Partner with our business users to understand their requirements and interactions with the data platform to ensure quality end data and product, with a smooth day-to-day operationWork as a key part of a highly-collaborative, tight-knit team of talented market data developersOwn the full scope of the assigned projects, including liaising with the business to gather requirements, designing the solution, and building and delivering completed functionality to productionProduce comprehensive written documentationBe a self-starter and perform with minimum supervision, exercising sound judgment QUALIFICATIONS & REQUIREMENTS: In order to effectively represent the Company and communicate with clients, the employee must be someone who has: Degree in Computer Science or STEM related subject2+ years development backgroundExpert knowledge of Python, with experience building complex, scalable systems and data manipulation (e.g. pandas/numpy/scikit-learn); C++, Rust knowledge would be a plus.Experience working in front-office within the Macro, Fixed Income or FX spaceStrong technical knowledge of systems supporting large, complex data sets in a performant, scalable mannerExperience with AWS technologies e.g. Redshift, SNS/SQS, S3, Lambdas and ECSExperience with data management principles and designing and developing data APIs, web services, ETL pipelinesUnix (Linux) and database experience (SQL, PostgreSQL, SQL Server)Familiarity with Bloomberg and/or Reuters/Refinitiv products a bonusAnalytical skills – Ability to troubleshoot and logically assess problems and determine solutionsDocumentation skills – ability to represent ideas, requirements, and problems in clear and concise documents.",15
"At Compass, we envision a world where the experience of selling or buying a home is simple and pleasant for everyone. Founded in 2012, Compass provides an end-to-end platform that empowers residential real estate agents to deliver exceptional service to their seller and buyer clients, all in service of our mission to help everyone find their place in the world.At Compass, we envision a world where the experience of selling or buying a home is simple and pleasant for everyone. Founded in 2012, Compass provides an end-to-end platform that empowers residential real estate agents to deliver exceptional service to their seller and buyer clients, all in service of our mission to help everyone find their place in the world.About The RoleCompass is looking for backend software engineers to help us build the future of real estate. We are building an end-to-end real estate technology platform that empowers agents and consumers. The AI team is responsible for building intelligent tools which help agents grow their business. We develop features which help agents prioritize their efforts and operate in an efficient manner. We are looking for someone with a proven track record of delivering critical backend components and building out customer facing features.At Compass You WillDeploy machine learning models into production, and support these models throughout their lifecycle.Collaborate closely with our machine learning scientists to move models from experimentation phase to production.Build out customer facing features that help agents make decisions and grow their business.Work with product managers to discern customer needs and translate these needs into an end product.What We Look For5+ years experience is preferred.Ability to collaborate with scientists, product management and work with an engineering-focused, iterative team to build and establish product requirements.Strong skills in coding and algorithms.Comfortable building prototypes from scratch.Experience working with a microservice based architecture.Experience with AWS development.BA/BS or MS in Computer Science/Machine Learning or equivalent and 5+ years experience is preferred.Do your best work, be your authentic self.At Compass, we believe that everyone deserves to find their place in the world — a place where they feel like they belong, where they can be their authentic selves, where they can thrive. Our collaborative, energetic culture is grounded in our Compass Entrepreneurship Principles and our commitment to diversity, equity, inclusion, growth and mobility. As an equal opportunity employer, we offer competitive compensation packages, robust benefits and professional growth opportunities aimed at helping to improve our employees' lives and careers.Notice for California Applicants",16
"URGENTLY HIRING SENIOR DATA ENGINEERS---Job Title: Senior Data EngineerJob Type: Direct-hire / PermanentSalary: $150K to $180K BaseLocation: 100% REMOTEFounded by former Google software engineers, we are the creator of one of the most widely-trusted distributed SQL databases that stores multiple copies of our clients' data in different cloud and regions to ensure uninterrupted access. Due to growth and demand we are in need of a talented Senior Data Engineer to join our diverse team!Priority will be given to candidates who complete the brief questionnaire within the application.Top Reasons to Work with UsHUGE growth opportunitiesOur company is valued at over $1 billionWe are a ""remote-first"" organizationWhat You Will Be DoingHelp to power data science, ETLs, and self-service data and tools to make us more efficient and help us to facilitate scalable decision-making.Design, build, and scale our backend data infrastructure.Define, develop, and manage curated datasets, key business metrics, and reporting across all functional business units.Work cross-organizationally with multiple teams and individuals.What You Need for this PositionBSCS or equivalent technical degree preferred5+ years of hands-on data engineering experienceExperience working directly with programming languages like Python, Java, Scala, or GoExperience with tools like Spark, Airflow, Presto, Hive, etc.Experience with reporting tools like Tableau, Looker, or similarExperience designing and implementing centralized data warehousing & platforms like Snowflake, Segment, etc.Strong knowledge of data architecture, data modeling, statistics, data science, and data infrastructure ecosystemsNice To HaveExperience with modern data warehouses, building scalable pipelines, and reporting/analytic techniques.Experience designing and developing data collecting and processing systems to handle large data sets.What's In It for YouCompetitive base salaryBonusHealth insurance 100% covered for you and your dependentsFlexible work hoursFlexible time offEducation reimbursementsPaid parental leaveFun company eventsMuch more!So if this sounds like you and you're interested in joining an incredible organization, please apply today!Colorado employees will receive paid sick leave. For additional information about available benefits, please contact Brennen LeftwickEmail Your Resume In Word ToLooking forward to receiving your resume through our website and going over the position with you. Clicking apply is the best way to apply, but you may also:Brennen.Leftwick@CyberCoders.comPlease do NOT change the email subject line in any way. You must keep the JobID: linkedin : BL1-1666550 -- in the email subject line for your application to be considered.***Brennen Leftwick - Recruiting Manager - CyberCodersApplicants must be authorized to work in the U.S.CyberCoders, Inc is proud to be an Equal Opportunity EmployerAll qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, protected veteran status, or any other characteristic protected by law.Your Right to Work - In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire.CyberCoders will consider for Employment in the City of Los Angeles qualified Applicants with Criminal Histories in a manner consistent with the requirements of the Los Angeles Fair Chance Initiative for Hiring (Ban the Box) Ordinance.",17
"Position Summary... What You'll Do...Here at Walmart, we're driven by an intellectual curiosity that keeps us on the cutting-edge of user design and a seamless customer experience. We're intrigued by the opportunity to engineer the most optimal approach that drives conversions and generates consumer loyalty across every touchpoint of the digital journey.Intelligent Retail Lab is part of Walmart's Global Tech, an innovation hub formed by the world's largest retailer focused on providing technology solutions that empower the business. IRL's mission is to create step change in-store experiences, leveraging emerging technology to help define and deliver on evolving customer expectations. Its success requires a cross-functional, mission-based team that is highly entrepreneurial, collaborative, and passionate about solving the unsolved problems.We are looking for a savvy Data Engineer to join our growing team of analytics experts. The Data Engineer will be responsible for expanding and optimizing our data and data pipelines, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.Responsibilities For Data EngineerCreate and maintain optimal data pipeline architecture,Assemble large, complex data sets that meet functional / non-functional business requirements.Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using BigQuery and CosmosDBBuild analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.Work with stakeholders including Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.Work with data and analytics experts to strive for greater functionality in our data systems.Qualifications For Data EngineerAdvanced working BigQuery knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.Strong analytic skills related to working with unstructured datasets.Build processes supporting data transformation, data structures, metadata, dependency and workload management.A successful history of manipulating, processing and extracting value from large disconnected datasets.Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.Experience supporting and working with cross-functional teams in a dynamic environment.Job Experience and EducationWe are looking for a candidate with 2-3+ years of experience in a Data Engineer role, who has attained a degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. They should also have experience using the following software/tools:Experience with relational SQL and NoSQL databasesExperience with data pipeline and workflow management toolsExperience with Node.js, GraphQL, and gRPC services a plusBenefits & PerksBeyond competitive pay, you can receive incentive awards for your performance. Other great perks include 401(k) match, stock purchase plan, paid maternity and parental leave, PTO, multiple health plans, and much more.Equal Opportunity EmployerWalmart, Inc. is an Equal Opportunity Employer – By Choice. We believe we are best equipped to help our associates, customers, and the communities we serve live better when we really know them. That means understanding, respecting, and valuing diversity- unique styles, experiences, identities, ideas, and opinions – while being inclusive of all people.About Global TechImagine working in an environment where one line of code can make life easier for hundreds of millions of people and put a smile on their face. That’s what we do at Walmart Global Tech. We’re a team of 15,000+ software engineers, data scientists and service professionals within Walmart, the world’s largest retailer, delivering innovations that improve how our customers shop and empower our 2.2 million associates. To others, innovation looks like an app, service, or some code, but Walmart has always been about people. People are why we innovate, and people power our innovations. Being human-led is our true disruption.Minimum Qualifications...Outlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications. As permitted by applicable law, provide evidence of full vaccination as defined by CDC guidelines OR secure approval of medical or religious accommodation for the vaccination mandate., Bachelor’s degree in Computer Science and 2 years’ experience in software engineering or related field OR 4 years’ experience in software engineering or related field. Preferred Qualifications...Outlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications. Masters: Computer Science Primary Location...121 River St, Hoboken, NJ 07030-5989, United States of America",18
"At Compass, we envision a world where the experience of selling or buying a home is simple and pleasant for everyone. Founded in 2012, Compass provides an end-to-end platform that empowers residential real estate agents to deliver exceptional service to their seller and buyer clients, all in service of our mission to help everyone find their place in the world.Compass is looking for a Staff Business/Data Analyst, supporting our Transaction Operations team, who is experienced in leveraging multiple complex data sources to derive a clear and actionable visualization of the data. This position will be responsible for developing metrics and reporting, understanding and managing data structure, strategic analysis, and rigorous deep dives to impact the business. They will work with technical teams, product teams, program teams, finance, and operations to drive reporting to visualize our success towards our goals, as well as to articulate productivity within the teams. This role will be responsible to map and define data parity across multiple business tools and processes, to ensure that data is consistent and complete. Additionally the role will be responsible for ensuring data continuity across platforms that have been sunset and replaced with new platforms, enabling a single source of truth for all of the historical and future reporting needs. This role requires an individual with excellent analytical abilities as well as outstanding business acumen and comfort with technical teams and systems. The successful candidate will be a self-starter comfortable with ambiguity, with strong attention to detail, and an ability to work in a fast-paced and ever-changing environment.If you are passionate about solving complex problems and thrive in a fast-paced environment, we'd be thrilled to meet you. This is a full-time position that will report directly to the Sr. Director of Program Management.At Compass You WillDrive the Transaction Operations Data analytics, working across multi-functional teams to prioritize roadmaps, improve processes, deliver clear and concise data reporting and insights.Drive standard business reporting processes, including weekly flashes, productivity reporting and any other data needed to support the Transaction Operations.Accountable for generating key business insights required to deliver against core input/output metric goals.Proactively seek to identify business opportunities and provide solutions based on a broad and deep knowledge of Compass data resources, industry best-practices, and work done by other teams.Simplify complex data structures to present clear, informative and actionable data visualizations.What We Look For+8 years of direct analytics experience, or equivalent combination of degree and experienceBA/BS degree in Engineering, Statistics, Computer Science, Operations Research, Business Analytics, Information Systems or related fieldAdvanced understanding and experience working with data warehousing, data quality, and data visualization.Advanced proficiency with Tableau, Snowflake, Data Bricks, Redshift and Google Sheets.Preferred experience with real estate industry tools such as SkySlope, PlanetRE, Profit Power as well as standard business tools such as NetSuite.Familiarity with SQL, relational databases. Python, and ETLMFamiliarity manipulating large sets of data, summarizing into metrics, and conducting appropriate statistical tests (e.g., regression, t-test, chi-square)Experience implementing basic software solutions to automate data source, visualization and/or data modeling applicationOutstanding communication skills - written and verbal - with the ability to tailor messaging strategy for different audiences.Outstanding organizational, project management, people and communication skills, including strong writing abilities and proven experience communicating key value messages to a variety of audiences both internal and external to Compass.Ability to prioritize, problem solve, multitask, and work independently in a dynamic, rapidly changing workplace with a bias towards action. Proficiency in sourcing and analyzing the data needed to make effective decisions as well as a high comfort level in operating with and making decisions with partial informationStrong soft skills and experience managing with empathy, including the ability to face challenges with a positive attitude.Do your best work, be your authentic self.At Compass, we believe that everyone deserves to find their place in the world — a place where they feel like they belong, where they can be their authentic selves, where they can thrive. Our collaborative, energetic culture is grounded in our Compass Entrepreneurship Principles and our commitment to diversity, equity, inclusion, growth and mobility. As an equal opportunity employer, we offer competitive compensation packages, robust benefits and professional growth opportunities aimed at helping to improve our employees' lives and careers.Notice for California Applicants",19
"Meta is looking for exceptionally talented and experienced engineers to join the GMS Technology team. Our team provides analytics and workflow tools for Global Marketing Solutions (GMS), partnering with sales, marketing, measurement, support and operations teams.In this role, you’ll work with some of the brightest minds in the industry, work with one of the richest data sets in the world, use cutting edge technology, and get an opportunity to solve some of the most challenging business and engineering problems, at a scale that few companies can match. You will do so by partnering with stakeholders/teams and building scalable, reliable solutions that provide business critical insights and metrics, while ensuring the best uptime and responsiveness.Manage data warehouse plans for a business vertical or a group of business verticalsBuild data expertise and own data quality for allocated areas of ownershipDesign, build, optimize, launch and support new and existing data models and analytical solutionsPartner with internal stakeholders to understand business requirements, work with cross-functional data and products teams and build efficient and scalable data solutionsConduct design and code reviewsWork with data infrastructure to triage infra issues and drive to resolutionManage the delivery of high impact dashboards, tools and data visualizations2+ years of experience in the data warehouse space, custom ETL design, implementation and maintenance2+ years of experience in SQL or similar languages, and development experience in at least one language (Python, PHP etc.)Experience with data architecture, data modeling, schema design and software developmentExperience in leading data driven projects from definition through interpretation and executionExperience with large data sets, Hadoop, and data visualization toolsExperience initiating and driving projects, and communicating data warehouse plans to internal clients/stakeholdersBachelor's degree in Computer Science, Computer Engineering, relevant technical field, or equivalent practical experienceExperience working in support of diverse communitiesFacebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities - we're just getting started.Facebook is proud to be an Equal Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law.Facebook is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com.",20
"Our client a well established hedge fund with 100B AUM is looking for a Data Analytics Python developer with AWS exp. They will be utilizing Python (Pandas, Python, Numpy, Scipy) for the development of a Data Analytics platform.. The successful candidate will need to have very solid programming skills and some financial instrament knowledge. Responsibilities:Help lead the thought process behind library development by proposing and researching new technologiesCode in Python to develop a brand new leading-edge analytics libraryContribute to the design efforts of the libraryWork with research teams to implement new strategiesBuild new visualization and reporting tools to help portfolio managers and researchers with model performanceQualifications:Graduate degree in Computer Science or a quantitative disciple from a top university preferredMinimum of 3+ years of professional experience with Python (Pandas, Python, Numpy, Scipy) developmentFinancial experience highly preferredStrong analytical, development, and modeling skillsIf interested, please apply with a copy of your resume and we will be in touch soon!",21
"About Swiss ReSwiss Re is one of the world’s leading providers of reinsurance, insurance and other forms of insurance-based risk transfer, working to make the world more resilient. We anticipate and manage a wide variety of risks, from natural catastrophes and climate change to cybercrime.At Swiss Re we combine experience with creative thinking and cutting-edge expertise to create new opportunities and solutions for our clients. This is possible thanks to the collaboration of more than 13,000 employees across the world.We offer a flexible working environment where curious and adaptable people thrive. Are you interested in joining us?About The TeamWith more than USD 120 bn of assets under management, Swiss Re Asset Management manages the assets generated through the core insurance and reinsurance business. With the advent of technologies such as cognitive computing, smart analytics or machine learning the reinsurance area will radically change in the next years in terms of distribution, underwriting and administration of insurance protection. As a recognized leader in the industry, Swiss Re is running numerous use cases and is building up research and development units utilizing state of the art digital analytics to offer actionable research and transformational opportunities.Asset Management IT ensures stable investment platforms and trade lifecycle management while delivering meaningful, timely financial data, enabling our customers to support AM's overall business performance. It also expands business processes automation and efficiency across the value chain through workflow solutions and robotics development.About The RoleAs Data Engineer in Asset Management, you will be working on key functional and technology initiatives leveraging cutting edge data, cloud and integration technologies. You will be involved in designing and developing end to end applications, dealing with database, client side and server-side components, and basic UI and integration of applications while working with some of the brightest and smartest people who are set to make an impact to Asset Management. This role can be based in NYC, London or Zurich.The ideal person will be someone who can both operate strategically and deliver tactical outcomes in complex projects, working side-by-side with our data scientists and other technology professionals. This person will have a broad understanding of multiple programming concepts, Agile methodology, and have proven him/herself leading development team in designing modern systems in multiple complex IT projects within the SDLC.Work with stakeholders and product, design, data scientist, quant, back-end, and front-end engineersAlign with business value streams and collaborate with the analytics squads on use-case design and deliveryHelp define and improve our standards for a maintainable, modular, and healthy code base for both the frontend and backendEvaluate and advocate for improvements in product design, quality, performance, and securitySolve technical problems of various levels of complexityTake ownership of code and confidently ship features, bug fixes, and improvements with support from other team members when neededStay on top of emerging trends and help the organization in making informed technology decisionsThis position demands an ability to problem solve, prioritize, and communicate proactively. The ideal candidate will have experience working backwards from the product design to understand the true requirements underlying feature requests.EssentialsMinimum 5+ years of work experience in an agile digital product development environment and BSc. or MBA/MS/PhD in computer science, engineering, information technologyExperience in multiple databases and understanding of traditional SQL and unstructured data repositories (experience with Elastic Search and MongoDB is considered a plus) Hands on experience in building applications for cloud platforms (Azure, AWS, GCP etc.), Azure preferredProficiency in Python, PySpark Experience in business intelligence platforms (Power BI, Tableau)Demonstrate familiarity with DevOps and Continuous Integration and Continuous Deployment concepts along-with necessary tools like Azure DevOps, Puppet, Chef, JIRA etc.Experience guiding development teams, in a high availability, high performance and cloud environmentUnderstanding of the Asset Management value chain and various products such as equities, fixed income, derivatives, private equity, private debt, and real estateAdvantageous Additional SkillsExperience in workflow automation tools (K2, Power Automate)Experience developing, integrating, and deploying solutions in Palantir Foundry Experience working with varied forms of data infrastructure inclusive of relational databases (MS SQL, Oracle DB, PostgreSQL) and non-relational (Hadoop HDFS, Mongo DB) is considered a plusExperience with Big Data frameworks (Elasticsearch, Spark, Kafka) is considered a plus Experience building front end applications using Angular/React and other JavaScript frameworksExperience building micro-services using Spring Boot, Node.JS, Spring Cloud etcAbility to design solutions in a multi-tier architecture, including database with UI componentHave a good understanding of various integration patters, REST protocols, and familiar with XML, JSONBehavioral CompetencesGreat team player with a ‘can do’ attitudeAbility to navigate through ambiguityAbility to work in an interdisciplinary and multi-cultural environmentHigh degree of flexibility, independent and proactive working styleAbility to work well under pressure and on multiple initiativesStrong dedication to quality and timely customer serviceKeywordsReference Code: 101945",22
