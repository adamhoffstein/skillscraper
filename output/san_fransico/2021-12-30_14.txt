
REMOTE Data Engineer needed for fast-growing SaaS company!This Jobot Job is hosted by James ThaiAre you a fit? Easy Apply now by clicking the "Apply" button and sending us your resume.Salary $80,000 - $110,000 per yearA Bit About UsBased in the beautiful city of Brentwood, MO, we are an international technology-driven SaaS company building enterprise web applications! Our company was founded in the mid-2000s in Ottowa, CA. We are expected to grow our company from 60 employees to 120 employees by the end of 2021. How are we going to accomplish this? By hiring the best developers in the greater St. Louis area who want to be a part of our company at an early stage! We are offering brand new development, a kick-ass work culture (excuse my French), and a relaxed work-life balance. You will be part of our foundational team of developers and will eventually manage your own team! We've got some VERY BIG plans and want you to be part of it!As our Data Engineer, you will build the infrastructure we need to leverage data throughout our organization. The ideal candidate is a visionary that can see the big picture and take a disciplined approach to methodically achieve the result needed. This individual is an independent worker who values autonomy. He/She brings deliberate thought and innovative thinking towards the problem or goal.This role is 100% remote and a direct/permanent position!Title  Data EngineerLocation  100% REMOTE | Work from home (WFH)Requirements SQL, ETL, building bit data pipelinesWhy join us? Competitive base salary $80,000 - $110,000 Full benefits Medical, Dental, Vision 401 (K) with generous company match Generous Paid time off (PTO) Vacation, sick, and paid holidays Life Insurance coverageJob Details Develop and build ETL/ELT data pipelines for use in data analysis. Create and maintain optimal data pipeline architecture. Keep our data separated and secure across multiple cloud environments. Assemble large, complex data sets that meet functional / non-functional business requirements. Deliver ad hoc and analytical reports to internal users and teams. Monitor and maintain ETL/ELT jobs and troubleshoot load issues. Manage change requests/ticket queues for analytical reports and ETL/ELT jobs. Ingest and transform structured, semi-structured and unstructured data from sources including relational databases, NoSQL, external APIs, JSON, XML, delimited files, and more. Work and deliver in agile methodology for new development projects. Deliver efficient and effective solutions on time. Ability to analyze and understand data source and design a data model for data capture and ETL/ ELT. Ability to identify bugs and apply fixes and check data quality via process/pipeline audits. Ability to work with team members, as well as cross-team for product delivery. Ability to work in agile environment with timely delivery of ETL/ ELT pipelines and reports. Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader Work with data and analytics experts to strive for greater functionality in our data systems. Identify, design, and implement internal process improvements automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. Work with stakeholders including the Executive, Product, Data, and Design teams to assist with data-related technical issues and support their data infrastructure needs.You have experience with some of the following Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases Experience building and optimizing ‘big data’ data pipelines, architectures and data setsExperience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement Strong analytic skills related to working with unstructured datasets Build processes supporting data transformation, data structures, metadata, dependency and workload management Three or more years of experience as a Data Engineer, Data Integration, Big Data, Business Intelligence, or Software Engineer Experience working with SQL, Python, C#, etc Strong project management and organizational skills Experience supporting and working with cross-functional teams in a dynamic environment Experience with big data tools Hadoop, Spark, Kafka a plus Experience with relational SQL and NoSQL databasesInterested in hearing more? Easy Apply now by clicking the "Apply" button.
