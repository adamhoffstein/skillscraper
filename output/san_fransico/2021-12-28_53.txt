
What you’ll do Work with a team that is making the transition to a Domain Driven Design architecture.Solve problems on a daily basis alongside talented engineers around data needs.Use Kafka to transport data between different domains.Respond to data requests from business analysts.Establish data transformation using the latest and greatest cloud technologies, including Confluent Kafka and Databricks.Data modelling for production applications.Assemble large, complex datasets from distributed OLTP systems for reportingBuild and optimize data sets, ‘big data’ data pipelines and architecturesImport and transform data from various data sources using cloud based and SQL technologiesDevelop in Python, Spark, T-SQL What you’ll need Bachelor’s Degree in Computer Science/Information Systems or equivalent10+ years’ experience working in a dataUnderstand data modeling and complex datasetsHave a working knowledge of popular development practices and development languagesHave experience building analytical tools to utilize data pipelines, providing actionable insight into key business performance metrics.Be able to support data infrastructure and data-related technical issuesBe able to support multiple data platforms (MS SQL, Kafka, MySQL, Databricks)Ability to perform root cause analysis on external and internal processes and data to identify opportunities for improvement and answer questionsUse your excellent analytic skills to provide answers using dataBuild processes that support data transformation, workload management, data structures, dependency and metadata.
