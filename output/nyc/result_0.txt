
DescriptionPulsePoint Data Engineering team plays a key role in our technology company that’s experiencing exponential growth. Our data pipeline processes over 80 billion impressions a day (> 20TB of data, 220 TB uncompressed). This data is used to generate reports, update budgets, and drive our optimization engines. We do all this while running against extremely tight SLAs and provide stats and reports as close to real-time as possible.The most exciting part about working at PulsePoint is the enormous potential for personal and professional growth. We are always seeking new and better tools to help us meet challenges such as adopting proven open-source technologies to make our data infrastructure more nimble, scalable and robust. Some of the cutting edge technologies we have recently implemented are Kafka, Spark Streaming, Presto, Airflow, and Kubernetes.What You'll Be DoingDesign, build and maintain reliable and scalable enterprise level distributed transactional data processing systems for scaling the existing business and supporting new business initiativesOptimize jobs to utilize Kafka, Hadoop, Presto, Spark Streaming and Kubernetes resources in the most efficient wayMonitor and provide transparency into data quality across systems (accuracy, consistency, completeness, etc)Increase accessibility and effectiveness of data (work with analysts, data scientists, and developers to build/deploy tools and datasets that fit their use cases)Collaborate within a small team with diverse technology backgroundsProvide mentorship and guidance to junior team membersTeam ResponsibilitiesInstallation, upkeep, maintenance and monitoring of Kafka, Hadoop, Presto, RDBMSIngest, validate and process internal & third party dataCreate, maintain and monitor data flows in Hive, SQL and Presto for consistency, accuracy and lag timeMaintain and enhance framework for jobs(primarily aggregate jobs in Hive) Create different consumers for data in Kafka using Spark Streaming for near time aggregationTrain Developers/Analysts on tools to pull dataTool evaluation/selection/implementationBackups/Retention/High Availability/Capacity PlanningReview/Approval - DDL for database, Hive Framework jobs and Spark Streaming to make sure they meet our standards24*7 On call rotation for Production supportTechnologies We UseAirflow - for job schedulingDocker - Packaged container image with all dependenciesGraphite/Beacon - for monitoring data flowsHive - SQL data warehouse layer for data in HDFSImpala- faster SQL layer on top of HiveKafka- distributed commit log storage Kubernetes - Distributed cluster resource managerPresto - fast parallel data warehouse and data federation layerSpark Streaming - Near time aggregationSQL Server - Reliable OLTP RDBMS Sqoop - Import/Export data to RDBMSRequirementsBA/BS degree in Computer science or related field5+ years of software engineering experienceFluency in Python, Experience in Scala/Java is a huge plus (Polyglot programmer preferred!)Proficiency in LinuxStrong understanding of RDBMS, SQL;Passion for engineering and computer science around dataKnowledge and exposure to distributed production systems i.e Hadoop is a huge plusKnowledge and exposure to Cloud migration is a plusWillingness to participate in 24x7 on-call rotationBenefitsComprehensive healthcare with 100%-paid medical, vision, life, & disability insurance401(k) MatchGenerous paid vacation and sick timeVacation reimbursement (we give you $500/year to take vacation), marriage leavePaid parental leave, new parent perks, & adoption assistanceAnnual training and development budget & annual tuition assistanceDonation matching and group volunteer opportunitiesA referral bonus program -- we love hiring referrals here at PulsePoint And there’s a lot more!Follow us on Glassdoor to learn more about what it’s like to work at PulsePoint!
